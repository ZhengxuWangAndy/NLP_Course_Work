{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Zhengxu Wang \n",
        "zhengxu@bu.edu  \n",
        "cs505 hw5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AQ4bfw3mERtM"
      },
      "source": [
        "# Programming Assignment Five: Spam detection with neural network.\n",
        "\n",
        "In this assignment, you are asked to build a neural network that can detect spam from a given SMS message.\n",
        "\n",
        "The provided files are:\n",
        "1. `spam_train.csv`: a csv file containing the training data, where the 'text' column provides the sms messages and the 'label' column indicates whether the sms message is a 'ham' (0) or a 'spam' (1).\n",
        "2. `spam_test.csv`: a csv file containing the testing data, following the same format as `spam_train.csv`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bDfTeToXFk9-"
      },
      "source": [
        "**Step 1: Compute the SMS message vector based on the average value of the word vectors that belong to the words in it.** \n",
        "\n",
        "Just like the last assignment, we compute the 'representation' of each message, i.e., the vector, by averaging word vectors with Word2Vec. But this time, we are using pre-trained [Glove word embeddings](https://nlp.stanford.edu/projects/glove/) instead. Specifically, we are using word embedding `glove.6B.100d` to obtain word vectors of each message, as long as the word is in the 'glove.6B.100d' embedding space.\n",
        "\n",
        "In other words, you need to:\n",
        "1. Have a [basic idea](https://nlp.stanford.edu/pubs/glove.pdf) of how Glove provides pre-trained word embeddings (vectors).\n",
        "2. Download and extract word vectors from `glove.6B.100d`, contained in `glove.6B.zip`.\n",
        "3. Compute the message vectors by averaging the vectors of words in the message."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df_train = pd.read_csv('./spam_train.csv')\n",
        "df_test = pd.read_csv('./spam_test.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>One small prestige problem now.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>Hey babe! I saw you came online for a second a...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>You should get more chicken broth if you want ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>My fri ah... Okie lor,goin 4 my drivin den go ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>That's cool, I'll come by like  &amp;lt;#&amp;gt; ish</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>995</th>\n",
              "      <td>995</td>\n",
              "      <td>Natalja (25/F) is inviting you to be her frien...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>996</th>\n",
              "      <td>996</td>\n",
              "      <td>Do you want a new Video handset? 750 any time ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>997</th>\n",
              "      <td>997</td>\n",
              "      <td>Congratulations ur awarded either a yrs supply...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>998</th>\n",
              "      <td>998</td>\n",
              "      <td>TheMob&gt;Yo yo yo-Here comes a new selection of ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999</th>\n",
              "      <td>999</td>\n",
              "      <td>Ur cash-balance is currently 500 pounds - to m...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1000 rows Ã— 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     Unnamed: 0                                               text  label\n",
              "0             0                    One small prestige problem now.      0\n",
              "1             1  Hey babe! I saw you came online for a second a...      0\n",
              "2             2  You should get more chicken broth if you want ...      0\n",
              "3             3  My fri ah... Okie lor,goin 4 my drivin den go ...      0\n",
              "4             4      That's cool, I'll come by like  &lt;#&gt; ish      0\n",
              "..          ...                                                ...    ...\n",
              "995         995  Natalja (25/F) is inviting you to be her frien...      1\n",
              "996         996  Do you want a new Video handset? 750 any time ...      1\n",
              "997         997  Congratulations ur awarded either a yrs supply...      1\n",
              "998         998  TheMob>Yo yo yo-Here comes a new selection of ...      1\n",
              "999         999  Ur cash-balance is currently 500 pounds - to m...      1\n",
              "\n",
              "[1000 rows x 3 columns]"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_train['text']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "vocab,embeddings = [],[]\n",
        "with open('./glove.6B/glove.6B.100d.txt','rt') as fi:\n",
        "    full_content = fi.read().strip().split('\\n')\n",
        "for i in range(len(full_content)):\n",
        "    i_word = full_content[i].split(' ')[0]\n",
        "    i_embeddings = [float(val) for val in full_content[i].split(' ')[1:]]\n",
        "    vocab.append(i_word)\n",
        "    embeddings.append(i_embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "400001"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "100"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(embeddings[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "400001"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "import spacy\n",
        "import re\n",
        "\n",
        "def cutWord(train_df):\n",
        "    allsents = []\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "    lemmatizer = nlp.get_pipe(\"lemmatizer\")\n",
        "    for i in range(len(train_df)):\n",
        "        train_df['text'][i] = re.sub(r'[\\[].*?[\\]]', '', train_df['text'][i])\n",
        "  \n",
        "        sentences = []\n",
        "        docParagraph = nlp(train_df['text'][i])\n",
        "        assert docParagraph.has_annotation(\"SENT_START\")\n",
        "        for sent in docParagraph.sents:\n",
        "            tokens = []\n",
        "            docSent = nlp(sent.text)\n",
        "            for token in docSent:\n",
        "                sentences.append(token.lemma_.lower())\n",
        "        allsents.append(sentences)\n",
        "    return allsents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/dq/nv7gkq6n28xbx7mghy5lgs300000gp/T/ipykernel_93689/3271232646.py:9: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  train_df['text'][i] = re.sub(r'[\\[].*?[\\]]', '', train_df['text'][i])\n"
          ]
        }
      ],
      "source": [
        "trainSents = cutWord(df_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['one', 'small', 'prestige', 'problem', 'now', '.']"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainSents[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/dq/nv7gkq6n28xbx7mghy5lgs300000gp/T/ipykernel_93689/3271232646.py:9: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  train_df['text'][i] = re.sub(r'[\\[].*?[\\]]', '', train_df['text'][i])\n"
          ]
        }
      ],
      "source": [
        "testSents = cutWord(df_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Turn to sentence vec\n",
        "import numpy as np\n",
        "import torch\n",
        "def build_sentence_vector(sentence,size):\n",
        "    sen_vec=np.zeros(size).reshape(size).astype(np.float32)\n",
        "    count=0\n",
        "    for word in sentence:\n",
        "        try:\n",
        "            sen_vec += np.array(word).reshape(size).astype(np.float32)\n",
        "            count+=1\n",
        "        except KeyError:\n",
        "            continue\n",
        "    if count!=0:\n",
        "        sen_vec/=count\n",
        "    return torch.from_numpy(sen_vec)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [],
      "source": [
        "trainVecs = []\n",
        "for i in range(len(trainSents)):\n",
        "    sentVecs = []\n",
        "    for token in trainSents[i]:\n",
        "        try:\n",
        "            sentVecs.append(embeddings[vocab.index(token)])\n",
        "        except:\n",
        "            pass\n",
        "    trainVecs.append(build_sentence_vector(sentVecs, 100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "100"
            ]
          },
          "execution_count": 62,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(trainVecs[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1000"
            ]
          },
          "execution_count": 63,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(trainVecs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {},
      "outputs": [],
      "source": [
        "testVecs = []\n",
        "for i in range(len(testSents)):\n",
        "    sentVecs = []\n",
        "    for token in testSents[i]:\n",
        "        try:\n",
        "            sentVecs.append(embeddings[vocab.index(token)])\n",
        "        except:\n",
        "            pass\n",
        "    testVecs.append(build_sentence_vector(sentVecs, 100))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X5FMwkD-KicD"
      },
      "source": [
        "**Step 2: Build 'dataset + data loader' that can feed data to train your model with Pytorch.**\n",
        "\n",
        "Our goal is to train a spam detection model (classification). Here's an [example](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html) of how a classfier is trained. Although it is for image classification, the idea is very similar:\n",
        "\n",
        "1. Prepare/build a dataset and load it with data loader;\n",
        "2. Prepare/build a model that takes the data input and predicts; and \n",
        "3. Prepare/build the optimizer and loss functions to train the model with the dataset.\n",
        "\n",
        "Naturally, the next thing to do is to prepare the data. We do it by building the 'Dataset' and 'Dataloader' with Pytorch.\n",
        "\n",
        "You may refer to [this page](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html) to get an idea of how to make 'Dataset' and 'Dataloader'. \n",
        "\n",
        "Hints:\n",
        "1. Make sure `__init__` , `__len__` and `__getitem__` of your defined dataset is implemented properly. In particular, the `__getitem__` function should return the specified message vector and its label.\n",
        "2. Don't compute the message vector when calling the `__getitem__` function, otherwise the training process will slow down A LOT.\n",
        "3. Make sure the shuffle is on for your data loader setup, as the data in the csv file is not. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, data, labels):\n",
        "        self.data = data\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        input = self.data[idx]\n",
        "        label = self.labels[idx]\n",
        "        return input,label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {},
      "outputs": [],
      "source": [
        "trainDataset = CustomDataset(trainVecs,df_train['label'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {},
      "outputs": [],
      "source": [
        "testDataset = CustomDataset(testVecs,df_test['label'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "trainDataloader = DataLoader(trainDataset, batch_size = 64, shuffle = True)\n",
        "testDataloader = DataLoader(testDataset, batch_size = 64, shuffle = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.linear1 = nn.Linear(100, 15)\n",
        "        self.activation = nn.ReLU()\n",
        "        self.linear2 = nn.Linear(15, 2)\n",
        "        self.softmax = nn.Softmax()\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.linear1(x)\n",
        "        x = self.activation(x)\n",
        "        x = self.linear2(x)\n",
        "        x = self.softmax(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "net = Net()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.0005, momentum=0.9)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Dav_HatOh8r"
      },
      "source": [
        "**Step 3: Build the neural net model.** \n",
        "\n",
        "Once the data is ready, we need to design and implement our neural network model.\n",
        "\n",
        "You should look [here](https://pytorch.org/tutorials/beginner/introyt/modelsyt_tutorial.html) to see how a model can be defined.\n",
        "\n",
        "The model does not need to be complicated. An example structure could be:\n",
        "\n",
        "1. linear layer 100 x 15\n",
        "2. ReLU activation layer\n",
        "3. linear layer 15 x 2 (think about why here is 2 instead of 1?)\n",
        "4. Softmax activation layer\n",
        "\n",
        "But feel free to test out other possible combinations of linear layers & activation functions and whether they make significant difference to the model performance later."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PkDTq-i9P4M_"
      },
      "source": [
        "**Step 4: Train the model with optimizer and loss function.**\n",
        "\n",
        "Lastly, we need to set up the [optimizer](https://pytorch.org/docs/stable/optim.html) and [loss function](https://pytorch.org/docs/stable/nn.html#loss-functions) to train the model. You may refer to the links for more details. Specifically, we need Stochastic Gradient Descent (SGD) for optimizer and CrossEntropyLoss for loss function.\n",
        "\n",
        "The last thing to do is to train the model for several epochs and evaluate its performance from time to time. For example,  train the model 5000 epochs, evaluating the model every 100 epochs. If you are not sure how the training works, you may refer to the [classification model tutorial](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html) to see how it is typically done. Don't forget to print the average loss of the epoch to see if the model is being optimized properly.\n",
        "\n",
        "The evaluation metric should be the [**accuracy**](https://en.wikipedia.org/wiki/Confusion_matrix) of predicting ham/spam on the testing data (TP+TN/(TP+TN+FP+FN)). The highest accuracy should be above at least **90%**. Try different settings of model structure, learning rate, and the number of training epochs  to achieve that level of accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "def eval(testloader, net):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    # since we're not training, we don't need to calculate the gradients for our outputs\n",
        "    with torch.no_grad():\n",
        "        for data in testloader:\n",
        "            images, labels = data\n",
        "            # calculate outputs by running images through the network\n",
        "            outputs = net(images)\n",
        "            # the class with the highest energy is what we choose as prediction\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    return 100 * correct // total\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/dq/nv7gkq6n28xbx7mghy5lgs300000gp/T/ipykernel_93689/2961610720.py:17: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  x = self.softmax(x)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 99 , Accuracy of the network on the 500 test texts: 76 % , loss: 0.005\n",
            "Epoch 199 , Accuracy of the network on the 500 test texts: 89 % , loss: 0.004\n",
            "Epoch 299 , Accuracy of the network on the 500 test texts: 89 % , loss: 0.004\n",
            "Epoch 399 , Accuracy of the network on the 500 test texts: 89 % , loss: 0.004\n",
            "Epoch 499 , Accuracy of the network on the 500 test texts: 90 % , loss: 0.003\n",
            "Epoch 599 , Accuracy of the network on the 500 test texts: 90 % , loss: 0.003\n",
            "Epoch 699 , Accuracy of the network on the 500 test texts: 91 % , loss: 0.003\n",
            "Finished Training\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(5000):  # loop over the dataset multiple times\n",
        "    acc = 0\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(trainDataloader, 0):\n",
        "        # get the inputs; data is a list of [inputs, labels]\n",
        "        inputs, labels = data\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # print statistics\n",
        "        running_loss += loss.item()\n",
        "            \n",
        "    if epoch % 100 == 99:\n",
        "        acc = eval(testDataloader, net)\n",
        "        print(f'Epoch {epoch} , Accuracy of the network on the 500 test texts: {acc} % , loss: {running_loss / 2000:.3f}')\n",
        "        running_loss = 0.0\n",
        "        if acc > 90:\n",
        "            break\n",
        "\n",
        "print('Finished Training')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.6 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
