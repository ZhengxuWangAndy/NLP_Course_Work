{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zhengxu Wang \n",
    "zhengxu@bu.edu  \n",
    "cs505 hw2\n",
    "## Task1\n",
    "Getting fishing and football data from tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import csv\n",
    "\n",
    "client = tweepy.Client(bearer_token='AAAAAAAAAAAAAAAAAAAAAGl1hAEAAAAAvHDZpENUinmoaM3e1EdQTyxEbtI%3D2SjBFtQBWxin1ZHoIXazuE22fNEvzdO3D3aTnzhfLS4pr1HWI3')\n",
    "query = 'fishing lang:en -has:mentions -has:links -is:retweet'\n",
    "tweets = list(tweepy.Paginator(client.search_recent_tweets, query=query, tweet_fields='created_at', max_results=100).flatten(limit=10000))\n",
    "\n",
    "driveFolderDirectory = './'\n",
    "savedFileName = 'tweetsFishing.csv'\n",
    "pathToSave = driveFolderDirectory + savedFileName\n",
    "\n",
    "with open(pathToSave, 'w', newline='') as csvfile:\n",
    "  fieldnames = ['idx','tweetId', 'tweetText']\n",
    "  writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "  writer.writeheader()\n",
    "  for i,tweet in enumerate(tweets):\n",
    "    writer.writerow({\n",
    "        'idx': i,\n",
    "        'tweetId': tweet.id,\n",
    "        'tweetText': tweet.data['text']\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'football lang:en -has:mentions -has:links -is:retweet'\n",
    "tweets = list(tweepy.Paginator(client.search_recent_tweets, query=query, tweet_fields='created_at', max_results=100).flatten(limit=10000))\n",
    "\n",
    "\n",
    "driveFolderDirectory = './'\n",
    "savedFileName = 'tweetsFootball.csv'\n",
    "pathToSave = driveFolderDirectory + savedFileName\n",
    "\n",
    "with open(pathToSave, 'w', newline='') as csvfile:\n",
    "  fieldnames = ['idx','tweetId', 'tweetText']\n",
    "  writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "  writer.writeheader()\n",
    "  for i,tweet in enumerate(tweets):\n",
    "    writer.writerow({\n",
    "        'idx': i,\n",
    "        'tweetId': tweet.id,\n",
    "        'tweetText': tweet.data['text']\n",
    "        })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rawTweetDictFish:  Genuinely lovely episode of Gone Fishing this week. Bob and Paul just giddy messing about up at Loch Ness. Glorious.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "def loadTextFromCSV(csvPath):\n",
    "  tweetDict = {}\n",
    "  with open(csvPath, newline='') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for row in reader:\n",
    "      tweetDict[int(row['idx'])] = row['tweetText']\n",
    "  return tweetDict\n",
    "\n",
    "#load your fishing tweet data here:\n",
    "csvPathFish = \"./tweetsFishing.csv\"\n",
    "rawTweetDictFish = loadTextFromCSV(csvPathFish)\n",
    "\n",
    "#print your tweet dictionary. You should see your saved tweets inside.\n",
    "print(\"rawTweetDictFish: \",rawTweetDictFish[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doing segmentation of sentence and tokenize, lower-case all words and shuffle data then split with 80% train data and 20% test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import random\n",
    "\n",
    "def preprocess(rawTweetDataDict,ngram):\n",
    "  #Input: a dictionary contains raw tweet data scraped from Tweeter\n",
    "\n",
    "  #Output: two lists of tweet sentences (train/test), but each tweet sentence is\n",
    "  #     represented in the form of tokens.\n",
    "  def sentenceSegmentation(tweet):\n",
    "    #Input: a string of raw tweet\n",
    "    #Output: a list of strings, each element in the list is a segmented sentence\n",
    "    sentences = sent_tokenize(tweet)\n",
    "    return sentences\n",
    "    \n",
    "  def sentenceLowerCase(sentence):\n",
    "    #Input: a string of sentence\n",
    "    #Output: a string of sentence, but all words in the sentence are lower-cased.\n",
    "    return sentence.lower()\n",
    "\n",
    "  def sentenceTokenization(sentence):\n",
    "    #Input: a string of sentence\n",
    "    #Output: a list of tokens that belong to the sentence.\n",
    "    tk = TweetTokenizer()\n",
    "    return tk.tokenize(sentence)\n",
    "  \n",
    "  dataSet = []\n",
    "  for i in range(len(rawTweetDataDict)):\n",
    "    sentences = sentenceSegmentation(rawTweetDataDict[i])\n",
    "    for sentence in sentences:\n",
    "      lower_sentence = sentenceLowerCase(sentence)\n",
    "      tokenizes = sentenceTokenization(lower_sentence)\n",
    "      dataSet.append(tokenizes)\n",
    "  random.shuffle(dataSet)\n",
    "  offset = int(len(dataSet) * 0.8)\n",
    "  return dataSet[0:offset], dataSet[offset:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainData, testData = preprocess(rawTweetDictFish,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['i', 'had', 'a', 'co', 'worker', 'who', 'came', 'in', 'to', 'work', 'sun', 'burnt', 'as', 'hell', 'â€¦', 'i', 'was', 'like', 'â€œ', 'did', 'i', 'not', 'tell', 'you', 'don', 'â€™', 't', 'take', 'yo', 'ass', 'out', 'there', 'fishing', '?'], ['no', 'matter', 'what', 'good', 'you', 'try', 'to', 'find', ',', 'the', 'only', 'thing', 'that', 'really', 'matters', 'is', 'the', 'win', '/', 'loss', 'column', 'â€¦', '.'], ['sigh', '...'], ['i', 'apologize', 'but', 'there', 'will', 'be', 'no', 'fox', 'stream', 'today', '.'], ['sometimes', 'fishing']]\n",
      "11569\n",
      "[[\"i'm\", 'fishing', 'on', 'the', 'river', 'today', 'ðŸŽ£'], ['happy', 'national', 'hunting', '&', 'fishing', 'day', 'from', 'sportsman', 'â€™', 's', 'guide', '!'], ['alright', ',', 'we', 'â€™', 've', 'got', 'a', 'lawn', 'chair', 'for', 'eating', 'food', 'rations', 'in', ',', 'we', 'got', 'a', 'sleeping', 'bag', 'to', 'store', 'food', 'rations', 'in', ',', 'we', 'have', 'food', 'rations', 'for', 'eating', 'food', 'rations', '.'], ['uh', '...', 'all', 'of', 'that', 'time', 'i', 'could', 'have', 'been', 'reading', '...', 'or', 'fishing', '...', 'or', 'napping', '...', 'or', '...'], ['why', 'y', 'â€™', 'all', 'bottoms', 'be', 'top', 'fishing']]\n",
      "2893\n"
     ]
    }
   ],
   "source": [
    "print(trainData[0:5])\n",
    "print(len(trainData))\n",
    "print(testData[0:5])\n",
    "print(len(testData))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Traning n-gram model with add-one smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here's a template you may want to start with\n",
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "from nltk.lm import MLE\n",
    "from nltk.lm import Laplace\n",
    "\n",
    "def trainNGramAddOneSmoothing(trainData,ngram):\n",
    "  # Input: a list of tweet sentences, each element is a list of tokens; n for ngram model\n",
    "  \n",
    "  train, vocab = padded_everygram_pipeline(order=ngram,text=testData)\n",
    "\n",
    "  # lm = MLE(ngram)\n",
    "  # ngram model with add-one smoothing\n",
    "  lm = Laplace(ngram)\n",
    "  lm.fit(train, vocab)\n",
    "  # Output: a n-gram model with add-one smoothing trained on your input data. \n",
    "  return lm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "triModel = trainNGramAddOneSmoothing(trainData,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['just', 'some', 'of', 'you', 'may', 'have', 'heard', 'the', 'story', 'start']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "triModel.generate(10,random_seed=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['marriage', '&', 'the', 'wailings', 'of', 'us', '!', '</s>', 'wild', 'petty']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biModel = trainNGramAddOneSmoothing(trainData,2)\n",
    "biModel.generate(10,random_seed=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['not', 'shit', 'thank', 'with', \"she's\", 'well', ',', 'i', 'with', 'on']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uniModel = trainNGramAddOneSmoothing(trainData,1)\n",
    "uniModel.generate(10,random_seed=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the average perplexity of my tri-gram model on the sentences of test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computePerplexity(model,testData):\n",
    "    # Input: your model; the testing data\n",
    "    # Output: average perplexity of the model on your testing data.\n",
    "    return model.perplexity(testData)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6132.265913071364\n"
     ]
    }
   ],
   "source": [
    "# compute \n",
    "# 1. the perplexity of your model on your testing data of 'fishing' tweets.\n",
    "print(computePerplexity(triModel, testData))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6749.414881966336\n"
     ]
    }
   ],
   "source": [
    "# 2. the perplexity of your model on your data of 'football' tweets.\n",
    "rawTweetDictFootball = loadTextFromCSV(\"./tweetsFootball.csv\")\n",
    "trainData_football, testData_football = preprocess(rawTweetDictFootball,3)\n",
    "footballData = trainData_football + testData_football\n",
    "print(computePerplexity(triModel, footballData))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why is there a difference between the two perplexities?**\n",
    "\n",
    "First, there are different dataset that contains different data and words. The same model meets the same words and will predict the same result of the next word. But in different sentences, they have different true word after the input word, which will affect the value of conditional probability.  \n",
    "Second, the dataset change from fishing to football, there will contain some words that are never included in the fishing data set. So, the model cannot recognize these words and would not generate a good prediction result.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating 10 tweets in 3 models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The uni-gram result\n",
      "this socrates guild community is go switched eating i'd me\n",
      "act though south cocaine in hoo online talents 40,\n",
      "yes women's ....these seriously paying engagement my my\n",
      "can't launch fishing's my notice . #fishpuncontest they competition came\n",
      "can; game and . getting we that spend brass\n",
      "not shit thank with she's well, i with on\n",
      "terrible the illustration confuse! over i somebody fly start\n",
      "favourite an one . kids fishing . is, he\n",
      "bust you a ref / chargers ðŸ¥¶ better oh i\n",
      "i focus afternoon to \" interest valley . like next\n",
      "The bi-gram result\n",
      "the repair shop . </s> fish tanks \" high-stick nymphing\n",
      "</s> their product . </s> for prospering #oceans, 123\n",
      "worst part . </s> the pursuit of great meetings with\n",
      "already have gained their original, $500 + </s>\n",
      "all because if i ask me too reading that don't\n",
      "marriage & the wailings of us! </s> wild petty\n",
      "ssn? </s> anyone? </s> go if it wasn\n",
      "catch a person, it in a lil bit my\n",
      "account specifically for sharks, cathy and doing some fishing\n",
      "gear long . </s> \" how to be justifying profiling\n",
      "The tri-gram result\n",
      "the repair shop & mortimer and whitehouse fishing show,\n",
      "</s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
      "with two 6 year olds fishing because my friend promised\n",
      "<s> completed the first set of weekly challenges, got\n",
      "<s> <s> i don â€™ t want to see if\n",
      "just some of you may have heard the story start\n",
      "shotgun to my fav swamps for fishing etc? </s>\n",
      "and combat follow the same is true of everything in\n",
      "<s> why can't florida and texas take care of the\n",
      "fish cleaving up through warm blue water, your perversity\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "# Here's a template you may want to start with\n",
    "\n",
    "def generateNewSentence(model,randomSeed):\n",
    "  # Input: your model; random seed that get you different generated sentence\n",
    "  sentence = model.generate(10, random_seed=randomSeed)\n",
    "  detokenizer = TreebankWordDetokenizer()\n",
    "  return detokenizer.detokenize(sentence)\n",
    "  # Output: a new sentence generated by your model, but in a string format instead of tokens.\n",
    "\n",
    "# Make loops to generate 10 tweets for each of your model (unigram, bigram and trigram)\n",
    "print(\"The uni-gram result\")\n",
    "for i in range(10):\n",
    "  print(generateNewSentence(uniModel,i))\n",
    "\n",
    "print(\"The bi-gram result\")\n",
    "for i in range(10):\n",
    "  print(generateNewSentence(biModel,i))\n",
    "\n",
    "print(\"The tri-gram result\")\n",
    "for i in range(10):\n",
    "  print(generateNewSentence(triModel,i))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here's a template you may want to start with\n",
    "\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import nltk\n",
    "from string import punctuation\n",
    "# need to download 'stopwords' before using it.\n",
    "# nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "def computeSentimentOfSentences(sentenceData, sentiment):\n",
    "  # Input: a list of sentences from tweets\n",
    "  Tweet = []\n",
    "  sum = 0.0\n",
    "  sid_obj = SentimentIntensityAnalyzer()\n",
    "  for i in range(len(sentenceData)):\n",
    "    # sentenceData[i] = removeStopWords(sentenceData[i])\n",
    "    sentiment_dict = sid_obj.polarity_scores(sentenceData[i])\n",
    "    sum += sentiment_dict['compound']\n",
    "    if sentiment_dict['compound'] >= 0.05 and sentiment == 'positive':\n",
    "      Tweet.append(sentenceData[i])\n",
    "    if sentiment_dict['compound'] <= -0.05 and sentiment == 'negative':\n",
    "      Tweet.append(sentenceData[i])\n",
    "  # Output: a list of sentences from positive tweets, average compound from all the input sentences\n",
    "  return Tweet, sum/len(sentenceData)\n",
    "\n",
    "def removeStopWords(sentence):\n",
    "  # Input: a sentence of tweet\n",
    "  stop_words = set(stopwords.words('english'))\n",
    "  filtered_sentence = [w for w in sentence if (w not in stop_words) and (w not in punctuation)]\n",
    "  cleanSentence = []\n",
    "  for word in filtered_sentence:\n",
    "      tempWord = re.sub('(\\W+)', '', word)\n",
    "      if tempWord != '':\n",
    "        cleanSentence.append(tempWord)\n",
    "  # Output: the sentence of input tweet, but non-stop + non-punctuation words are removed\n",
    "  return cleanSentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Compute the ratios of positive and negative sentences in your collected data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive ratio : 2.869589268427603%\n",
      "Negative ratio : 1.9637671138155166%\n"
     ]
    }
   ],
   "source": [
    "fishingData = trainData + testData\n",
    "positiveList, averageCompoundFish = computeSentimentOfSentences(fishingData,'positive')\n",
    "negativeList, averageCompoundFish = computeSentimentOfSentences(fishingData,'negative')\n",
    "print(\"Positive ratio : \" + str(len(positiveList) / len(fishingData) * 100) + \"%\")\n",
    "print(\"Negative ratio : \" + str(len(negativeList) / len(fishingData) * 100) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. compute the average compound of the collected data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average compound in fishing data : 0.006148796846909132\n",
      "Average compound in football data : 0.009423123443614376\n"
     ]
    }
   ],
   "source": [
    "positiveListFootball, averageCompoundFootball = computeSentimentOfSentences(footballData,'positive')\n",
    "print(\"Average compound in fishing data : \" + str(averageCompoundFish))\n",
    "print(\"Average compound in football data : \" + str(averageCompoundFootball))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think they are generally neutral, but the average compound exceed 0 means there are more positive tweets than the negative tweets. So, maybe we can also think it is genrally positive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Compute the top 10 non stop words from positive tweets of 'fishing'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "allWordsDict = {}\n",
    "for sentence in positiveList:\n",
    "    tempList = removeStopWords(sentence)\n",
    "    for word in tempList:\n",
    "        if word not in allWordsDict.keys():\n",
    "            allWordsDict[word] = 0\n",
    "        else:\n",
    "            allWordsDict[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are the top 10 words below\n",
      "fishing\n",
      "go\n",
      "like\n",
      "win\n",
      "im\n",
      "going\n",
      "life\n",
      "ÙƒÙˆØ¯\n",
      "Ø®ØµÙ…\n",
      "Ù±Û’Ù‡Û’Ø±Ø¨Ù€\n"
     ]
    }
   ],
   "source": [
    "top10 = sorted(allWordsDict.items(), key=lambda dict: dict[1], reverse=True)[0:10]\n",
    "print('Here are the top 10 words below')\n",
    "for i in range(10):\n",
    "    print(top10[i][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The garbled words above are caused by the emoji during processing."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
