{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Zhengxu Wang \n",
        "zhengxu@bu.edu  \n",
        "cs505 hw3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Un_XAhJ34Js5"
      },
      "source": [
        "In this piece of code, we are going to process and analyze the data we collect from Twitter, Wikipedia, ABC and Fox news.\n",
        "\n",
        "Prior to this assignment, please make sure you have implemented the scraping functions so that you could scrap data from the Wikipedia, ABC and Fox news pages.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3-uVC6va2y5"
      },
      "source": [
        "Task 1. With your implemented code provided in the first lab section, get the \"article\" texts of the wikipedia page of \"fishing\" and its all linked wiki pages. Your saved data should contain the titles of the wiki pages and their article texts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import time # for setting up a delay on getting htmls from wiki server.\n",
        "from tqdm import tqdm\n",
        "def getPageFromWiki(url):\n",
        "    r = requests.get(url)\n",
        "    soup = BeautifulSoup(r.content, 'html.parser')\n",
        "    return soup\n",
        "\n",
        "def getHeading(soup):\n",
        "    return soup.title.string\n",
        "\n",
        "\n",
        "#mw-content-text\n",
        "#bodyContent\n",
        "#mw-content-text > div.mw-parser-output\n",
        "def getContent(soup):\n",
        "    temp = soup.select_one('#mw-content-text > div.mw-parser-output')\n",
        "    if temp == None:\n",
        "        return None\n",
        "    return temp.get_text()\n",
        "\n",
        "    \n",
        "def getLinks(soup):\n",
        "\n",
        "  linksDict = {}\n",
        "  for link in soup.select_one('#mw-content-text > div.mw-parser-output').find_all('a'):\n",
        "    title = link.get('title')\n",
        "    url = link.get('href')\n",
        "    if title != None and url != None:\n",
        "        if url[0] == '/':\n",
        "            linksDict[title] = 'https://en.wikipedia.org' + url\n",
        "\n",
        "  return linksDict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "a set of 422 links are found.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 422/422 [09:57<00:00,  1.42s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "a size of 372 content dictionary is built.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "#Lastly, write them down in a .csv file for both the abc and fox news. \n",
        "\n",
        "import csv\n",
        "\n",
        "pathToSave = 'wikiContents.csv'\n",
        "\n",
        "# Once you've implemented the above functions, run the following piece to see if a dictionary that contains the wiki articles we scraped.\n",
        "# Run a for loop to get all the article contents from Wikipedia.\n",
        "pageDict = {}\n",
        "\n",
        "page = getPageFromWiki('https://en.wikipedia.org/wiki/Fishing') # scrap the main page we want. \n",
        "header = getHeading(page)\n",
        "content = getContent(page)\n",
        "pageDict[header] = content\n",
        "# print(pageDict)\n",
        "\n",
        "linksDict = getLinks(page) # get the links contained in the article part of the page.\n",
        "print(\"a set of {} links are found.\".format(len(linksDict)))\n",
        "\n",
        "for title in tqdm(list(linksDict.keys())): # set up a loop to , set a delay at each iteration\n",
        "  url = linksDict[title]\n",
        "  page = getPageFromWiki(url)\n",
        "  header = getHeading(page)\n",
        "  content = getContent(page)\n",
        "  if content != None:\n",
        "    pageDict[header] = content\n",
        "  time.sleep(1) # Remember to set a delay >=1 second so you won't break the server.\n",
        "\n",
        "print(\"a size of {} content dictionary is built.\".format(len(pageDict)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Lastly, save your contents and corresponding title in a .csv file.\n",
        "import csv\n",
        "\n",
        "pathToSave = 'wikiContents.csv'\n",
        "\n",
        "with open(pathToSave, 'w', newline='') as csvfile:\n",
        "  fieldnames = ['idx','wikiTitle', 'wikiContents']\n",
        "  writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "  writer.writeheader()\n",
        "  for i,wikiContentKey in enumerate(pageDict.keys()):\n",
        "    writer.writerow({'idx': i, 'wikiTitle': wikiContentKey,'wikiContents': pageDict[wikiContentKey]})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "Q5VyckRsTLUW"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import csv\n",
        "csv.field_size_limit(sys.maxsize)\n",
        "\n",
        "# Here is a function you could load the text data if your saved data follows\n",
        "# the format we provide the in first lab section code.\n",
        "\n",
        "def loadWikiTexts(csvPath):\n",
        "  wikiRawTextDict = {}\n",
        "  with open(csvPath, newline='') as csvfile:\n",
        "    reader = csv.DictReader(csvfile)\n",
        "    for row in reader:\n",
        "      wikiRawTextDict[row['wikiTitle']] = row['wikiContents']\n",
        "  return wikiRawTextDict\n",
        "\n",
        "# Load your wiki text data here\n",
        "\n",
        "wikiRawDict = loadWikiTexts('./wikiContents.csv')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GCatO_lkThcP"
      },
      "source": [
        "Task 2. With library Spacy and Regular Expression (re), preprocess our scraped data to:\n",
        "\n",
        "- Remove all the references texts [...] in the scraped data  ([re](https://docs.python.org/3/library/re.html)). \n",
        "- [Sentence split](https://spacy.io/usage/linguistic-features#sbd) (Spacy).\n",
        "- [Tokenize](https://spacy.io/usage/linguistic-features#tokenization) (Spacy)\n",
        "- [Lemmatize](https://spacy.io/usage/linguistic-features#lemmatization) (Spacy)\n",
        "- [Lower case](https://www.programiz.com/python-programming/methods/string/lower) (String)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "76c1vkPHgIz6"
      },
      "outputs": [],
      "source": [
        "# install spacy and related package(s)\n",
        "\n",
        "!pip3 install -U pip setuptools wheel\n",
        "!pip3 install -U spacy\n",
        "!python3 -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "e9gvNMaZUqfo"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import spacy\n",
        "\n",
        "def preprocess(wikiTextDict):\n",
        "  # Input: a wiki text dictionary with keys are titles and values are the corresponding texts.\n",
        "  # Output: a wiki text dictionary with keys are the titles and the values are the preprocessed texts \n",
        "  # (sentences - tokens).\n",
        "\n",
        "  # sub-task 1: remove all the references texts \"[...]\"\n",
        "\n",
        "  nlp = spacy.load(\"en_core_web_sm\")\n",
        "  lemmatizer = nlp.get_pipe(\"lemmatizer\")\n",
        "  for key in wikiTextDict.keys():\n",
        "    wikiTextDict[key] = re.sub(r'[\\[].*?[\\]]', '', wikiTextDict[key])\n",
        "    \n",
        "  # sub-task 2: segment all the sentences in the wiki texts.\n",
        "    sentences = []\n",
        "    docParagraph = nlp(wikiTextDict[key])\n",
        "    assert docParagraph.has_annotation(\"SENT_START\")\n",
        "    for sent in docParagraph.sents:\n",
        "      # sub-task 3: tokenize the sentences from sub-task 2.\n",
        "      # sub-task 4: lemmatize the tokens from sub-task 3.\n",
        "      # sub-task 5: lower-case the tokens from sub-task 3/4.\n",
        "\n",
        "      tokens = []\n",
        "      docSent = nlp(sent.text)\n",
        "      for token in docSent:\n",
        "        tokens.append(token.lemma_.lower())\n",
        "      sentences.append(tokens)\n",
        "    wikiTextDict[key] = sentences\n",
        "\n",
        "  return wikiTextDict\n",
        "  # You don't need to follow the order of the sub-tasks.\n",
        "\n",
        "\n",
        "# Preprocess your data here.\n",
        "wikiProcessedDict = preprocess(wikiRawDict)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['activity',\n",
              " 'of',\n",
              " 'try',\n",
              " 'to',\n",
              " 'catch',\n",
              " 'fish',\n",
              " '\\n',\n",
              " 'for',\n",
              " 'other',\n",
              " 'use',\n",
              " ',',\n",
              " 'see',\n",
              " 'fishing',\n",
              " '(',\n",
              " 'disambiguation',\n",
              " ')',\n",
              " '.',\n",
              " '\\n\\n\\n']"
            ]
          },
          "execution_count": 66,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "wikiProcessedDict['Fishing - Wikipedia'][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zgosL6GBUrav"
      },
      "source": [
        "Task 3. Construct a dictionary of the vocabulary for your scraped data (all texts). The keys are the word types and the values are the count of the appearances of the word (frequency)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "cjbzs5IsVCC5"
      },
      "outputs": [],
      "source": [
        "def computeFreq(wikiTextDict):\n",
        "\n",
        "  # Input: a wiki text dictionary with keys are titles and values are the preprocessed corresponding texts.\n",
        "  # Output: a dictionary with keys are the word types, and the values are the appearance counts of the word types\n",
        "  tokenDict = {}\n",
        "  for key in wikiTextDict.keys():\n",
        "    for sentence in wikiTextDict[key]:\n",
        "      for token in sentence:\n",
        "        if token.isalpha() == True:\n",
        "          if token in tokenDict.keys():\n",
        "            tokenDict[token] += 1\n",
        "          else:\n",
        "            tokenDict[token] = 0\n",
        "  return tokenDict\n",
        "\n",
        "tokenDict = computeFreq(wikiProcessedDict)\n",
        "  # Compute the frequency dictionary here.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "check result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "('activity', 713)\n",
            "('of', 65963)\n",
            "('try', 159)\n",
            "('to', 33098)\n",
            "('catch', 1572)\n",
            "('fish', 10217)\n",
            "('for', 13647)\n",
            "('other', 4376)\n",
            "('use', 6696)\n",
            "('see', 2340)\n"
          ]
        }
      ],
      "source": [
        "i = 0\n",
        "for item in tokenDict.items():\n",
        "    print(item)\n",
        "    i += 1\n",
        "    if i == 10:\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fogf5jRAVJr3"
      },
      "source": [
        "Task 4. What are the top 20 non-stop, non-punctuation words in the vocabulary according to frequency?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "3FX_TP8JVRxd"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import stopwords\n",
        "\n",
        "def computeTop20Words(freqDict,stop_words):\n",
        "  \n",
        "  # Input: a dictionary with keys are the word types, and the values are the appearance counts of the word types\n",
        "  # Output: a list of 20 words that appear most frequently in all the preprocessed scraped texts.\n",
        "\n",
        "  # If not preprocessed specifically, the punctuations still exist in the frequency dictionary from task 3.\n",
        "  # You need to remove them before outputing the top 20 words.\n",
        "  Top20 = []\n",
        "  sortedList = sorted(freqDict.items(), key=lambda dict: dict[1], reverse=True)\n",
        "  while True:\n",
        "    if len(Top20) == 20:\n",
        "      break\n",
        "    key = sortedList.pop(0)[0]\n",
        "    if key not in stop_words:\n",
        "      Top20.append(key)\n",
        "\n",
        "  return Top20\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "Top20 = computeTop20Words(tokenDict, stop_words)\n",
        "# Print your top 20 words here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['fish',\n",
              " 'retrieve',\n",
              " 'fishing',\n",
              " 'use',\n",
              " 'water',\n",
              " 'archive',\n",
              " 'also',\n",
              " 'original',\n",
              " 'world',\n",
              " 'may',\n",
              " 'new',\n",
              " 'isbn',\n",
              " 'sea',\n",
              " 'include',\n",
              " 'one',\n",
              " 'b',\n",
              " 'marine',\n",
              " 'specie',\n",
              " 'large',\n",
              " 'year']"
            ]
          },
          "execution_count": 82,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Top20"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-Pvheg3VZ72"
      },
      "source": [
        "Task 5. Use library such as wordcloud, [generate the word cloud](https://towardsdatascience.com/simple-wordcloud-in-python-2ae54a9f58e5) of the text to visualize the distribution of non-stop and non-punctuation words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1dZdEYCNVk3u"
      },
      "outputs": [],
      "source": [
        "from wordcloud import WordCloud, STOPWORDS\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plotWordCloud(image):\n",
        "  # Input: word cloud image\n",
        "  # Output: (display the cloud image in the output)\n",
        "\n",
        "  plt.figure(figsize=(40, 30))\n",
        "  # Display image\n",
        "  plt.imshow(image) \n",
        "  # No axis details\n",
        "  plt.axis(\"off\")\n",
        "\n",
        "def generateWordCloud(text):\n",
        "  # Input: all texts in the scraped wiki data.\n",
        "  # Output: word cloud image.\n",
        "  wordcloud = WordCloud(width= 3000, height = 2000, random_state=1, background_color='salmon', colormap='Pastel1', collocations=False, stopwords = STOPWORDS).generate(text)\n",
        "\n",
        "  return wordcloud\n",
        "\n",
        "# Draw your word cloud here\n",
        "\n",
        "allWordList = []\n",
        "for paragraph in wikiProcessedDict.values():\n",
        "    for sent in paragraph:\n",
        "        for word in sent:\n",
        "            if word not in stop_words and word.isalpha() == True:\n",
        "                allWordList.append(word)\n",
        "\n",
        "len(allWordList)\n",
        "\n",
        "wordcloud = generateWordCloud(' '.join(allWordList))\n",
        "plotWordCloud(wordcloud)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vRhl_gmoVl7n"
      },
      "source": [
        "Task 6. Preprocess the raw scraped tweets with keyword ’fishing’ you’ve collected in the last assignment in the same way as you preprocess the wiki texts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "a1SoVTjHWGUX"
      },
      "outputs": [],
      "source": [
        "# Here is a function you could load the tweet text data if your saved data follows\n",
        "# the format we provide the in last lab section code.\n",
        "\n",
        "import csv\n",
        "\n",
        "def loadTweetTextFromCSV(csvPath):\n",
        "  tweetDict = {}\n",
        "  with open(csvPath, newline='') as csvfile:\n",
        "    reader = csv.DictReader(csvfile)\n",
        "    for row in reader:\n",
        "      tweetDict[int(row['idx'])] = row['tweetText']\n",
        "  return tweetDict\n",
        "\n",
        "# processedTweetData = preprocess(tweetDict) # here we assume you have implemented the preprocess function in task 2.\n",
        "tweetRawDict = loadTweetTextFromCSV('./tweetsFishing.csv')\n",
        "processedTweetData = preprocess(tweetRawDict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[['genuinely',\n",
              "  'lovely',\n",
              "  'episode',\n",
              "  'of',\n",
              "  'gone',\n",
              "  'fishing',\n",
              "  'this',\n",
              "  'week',\n",
              "  '.'],\n",
              " ['bob',\n",
              "  'and',\n",
              "  'paul',\n",
              "  'just',\n",
              "  'giddy',\n",
              "  'mess',\n",
              "  'about',\n",
              "  'up',\n",
              "  'at',\n",
              "  'loch',\n",
              "  'ness',\n",
              "  '.'],\n",
              " ['glorious', '.']]"
            ]
          },
          "execution_count": 93,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "processedTweetData[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5a612hyWGdX"
      },
      "source": [
        "Task 7. Compute how many **word types** in your tweets are out-of-vocabulary (out of Wiki vocabulary Dict), divided by the number of **word types** in your tweets. Show the value in percentage (%)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "id": "Al98KhJQWURv"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "33.166058394160586%\n"
          ]
        }
      ],
      "source": [
        "def computeOOVWordTypes(tweetVocabDict, wikiVocabDict):\n",
        "\n",
        "  # Input: a dictionary of tweet data vocabulary, a dictionary of wiki data vocabulary.\n",
        "  # Output: the ratio of word types in your tweets that are out-of-vocabulary w.r.t. wiki vocabulary\n",
        "  # v.s. total number of word types in your tweet data.\n",
        "\n",
        "  # The ratio should be in percentage.\n",
        "  count = 0\n",
        "  for key in tweetVocabDict.keys():\n",
        "    if key not in wikiVocabDict.keys():\n",
        "      count += 1\n",
        "  return str(count/len(tweetVocabDict.keys()) * 100) + '%'\n",
        "# Print your ratio here.\n",
        "tweetVocabDict = computeFreq(processedTweetData)\n",
        "print(computeOOVWordTypes(tweetVocabDict, tokenDict))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mqMB73wWehZ"
      },
      "source": [
        "Task 8. Compute how many **word tokens** in your tweets are out of vocabulary, divided by the number of **word tokens** in your tweets. (This is the OOV-rate of your tweet test set.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "id": "nZ-yPMmXWnqx"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.396296632477081%\n"
          ]
        }
      ],
      "source": [
        "def computeOOVWordTokens(tweetVocabDict, wikiVocabDict):\n",
        "\n",
        "  # Input: a dictionary of tweet data vocabulary, a dictionary of wiki data vocabulary. (E.g. computed from task 3)\n",
        "  # Output: the ratio of word tokens in your tweets that are out-of-vocabulary w.r.t. wiki vocabulary\n",
        "  # v.s. total number of word tokens in your tweet data.\n",
        "\n",
        "  # Remeber this time we count the number of tokens instead of types. The ratio should be in percentage.\n",
        "  count = 0\n",
        "  sum = 0\n",
        "  for key in tweetVocabDict.keys():\n",
        "    sum += tweetVocabDict[key]\n",
        "    if key not in wikiVocabDict.keys():\n",
        "      count += tweetVocabDict[key]\n",
        "  return str(count/sum * 100) + '%'\n",
        "\n",
        "# Print your ratio here.\n",
        "print(computeOOVWordTokens(tweetVocabDict, tokenDict))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nyEhREl4WqTJ"
      },
      "source": [
        "Task 9. Get the first 9,000 sentences from the processed Wikipedia data from task 2, train a trigram Add-one-smoothing language model based\n",
        "on these 9,000 sentences (which you should have done so in the last assignment). \n",
        "\n",
        "(You could consider using the language model from NLTK.)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get the first 9000 sentences from the processed wiki data.\n",
        "wikiSentences_9k = []\n",
        "for value in wikiProcessedDict.values():\n",
        "    for sent in value:\n",
        "        sentence = []\n",
        "        for word in sent:\n",
        "            if word.isalpha() == True:\n",
        "                sentence.append(word)\n",
        "        if sentence != []:\n",
        "            wikiSentences_9k.append(sentence)\n",
        "        if len(wikiSentences_9k) == 9000:\n",
        "            break\n",
        "    if len(wikiSentences_9k) == 9000:\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['this',\n",
              " 'conclusion',\n",
              " 'be',\n",
              " 'base',\n",
              " 'on',\n",
              " 'the',\n",
              " 'lobster',\n",
              " 'simple',\n",
              " 'nervous',\n",
              " 'system']"
            ]
          },
          "execution_count": 112,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "wikiSentences_9k[8999]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {
        "id": "TfrU29ZNXi15"
      },
      "outputs": [],
      "source": [
        "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
        "from nltk.lm import MLE\n",
        "from nltk.lm import Laplace\n",
        "\n",
        "def trainLanguageModel(processedWikiData):\n",
        "  # Input: the pre-processed wiki data\n",
        "  # Output: a trigram model trained on the wiki data \n",
        "  \n",
        "  # You could refer to the last assignment to implement this function.\n",
        "  train, vocab = padded_everygram_pipeline(order=3, text= processedWikiData)\n",
        "  lm = Laplace(3)\n",
        "  lm.fit(train, vocab)\n",
        "  return lm\n",
        "\n",
        "# Train the language model with the processed data.\n",
        "wikiLanguageModel = trainLanguageModel(wikiSentences_9k)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Test the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['mastodon',\n",
              " 'plano',\n",
              " 'transverse',\n",
              " 'arrowhead',\n",
              " 'systems',\n",
              " 'game',\n",
              " 'drive',\n",
              " 'system',\n",
              " 'buffalo',\n",
              " 'jump']"
            ]
          },
          "execution_count": 117,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "wikiLanguageModel.generate(10, random_seed=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K5wTbC75XolI"
      },
      "source": [
        "Task 10. Report the average perplexity of this Wikipedia-trained language model on your processed Twitter test sentences (i.e. the 20% split) related to \"fishing\". Compare this perplexity to the one you obtained in task 4 of the last assignment, specifically, the trigram LM trained on tweets. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get the 20% sentences from the processed tweet data.\n",
        "tweetAllData = []\n",
        "for value in processedTweetData.values():\n",
        "    for sent in value:\n",
        "        sentence = []\n",
        "        for word in sent:\n",
        "            if word.isalpha() == True:\n",
        "                sentence.append(word)\n",
        "        if sentence != []:\n",
        "            tweetAllData.append(sentence)\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare the testing data\n",
        "tweetTestData = tweetAllData[0:int(len(tweetAllData)*0.2)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "2974"
            ]
          },
          "execution_count": 127,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(tweetTestData)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['genuinely', 'lovely', 'episode', 'of', 'gone', 'fishing', 'this', 'week']"
            ]
          },
          "execution_count": 129,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tweetTestData[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "metadata": {
        "id": "isI3myc6XtW3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "13170.05302785912\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def computePerplexity(model,testData):\n",
        "  \n",
        "  # Input: your model; the testing data\n",
        "\n",
        "  # Output: average perplexity of the model on your testing data.\n",
        "\n",
        "  # You may want to re-use the same function you implemented in the last assignment\n",
        "  sum = 0\n",
        "  for sent in testData:\n",
        "    sum += model.perplexity(sent)\n",
        "\n",
        "  return sum/len(testData)\n",
        "\n",
        "# Compute and print the average perplexity of the wiki-trained model on your tweet testing data.\n",
        "print(computePerplexity(wikiLanguageModel, tweetTestData))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jSjI092zXtov"
      },
      "source": [
        "Task 11. Scrap 100 news from both ABC news and Fox news (100 each) with the code provided in the first lab section. Preprocess the texts in the same way as task 2."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 271,
      "metadata": {},
      "outputs": [],
      "source": [
        "def getPageFrom(url):\n",
        "    r = requests.get(url)\n",
        "    soup = BeautifulSoup(r.content, 'lxml-xml')\n",
        "    return soup\n",
        "def getUrlList(sitemap):\n",
        "  # This function should return a list of URLs of news contained in the sitemap page.\n",
        "    url_list = []\n",
        "    for link in sitemap.find_all('loc'):\n",
        "        url_list.append(link.text)\n",
        "    return url_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Get the CNN links and Fox links and ABC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 276,
      "metadata": {},
      "outputs": [],
      "source": [
        "ABCNewsSitemap = getPageFrom('https://abcnews.go.com/xmlLatestStories')\n",
        "ABCNewsLinks = getUrlList(ABCNewsSitemap)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 272,
      "metadata": {},
      "outputs": [],
      "source": [
        "CNNNewsSitemap = getPageFrom('https://www.cnn.com/sitemaps/cnn/news.xml')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 273,
      "metadata": {},
      "outputs": [],
      "source": [
        "CNNNewsLinks = getUrlList(CNNNewsSitemap)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 274,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "530"
            ]
          },
          "execution_count": 274,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(CNNNewsLinks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 188,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'https://www.cnn.com/2022/10/05/asia/north-korea-missile-intl/index.html'"
            ]
          },
          "execution_count": 188,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "CNNNewsLinks[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 293,
      "metadata": {},
      "outputs": [],
      "source": [
        "FOXNewsSitemap = getPageFrom('https://www.foxnews.com/sitemap.xml?type=news')\n",
        "FOXNewsLinks = getUrlList(FOXNewsSitemap)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 289,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "356"
            ]
          },
          "execution_count": 289,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(FOXNewsLinks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Get Fox news dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 285,
      "metadata": {},
      "outputs": [],
      "source": [
        "from newspaper import Article\n",
        "def getFOXNewsDict(url_list):\n",
        "\n",
        "  # Your key should be the news title and value should be the article text of the news.\n",
        "  newsDict = {}\n",
        "  # IMPLEMENT YOUR CODE HERE:# \n",
        "  for url in url_list:\n",
        "    article = Article(url)\n",
        "    article.download()\n",
        "    article.parse()\n",
        "    title = article.title\n",
        "    text = article.text\n",
        "    if title != '' and text != '':\n",
        "        newsDict[title] = text\n",
        "    \n",
        "    if len(newsDict) == 14:\n",
        "        break\n",
        "  \n",
        "  return newsDict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 212,
      "metadata": {},
      "outputs": [],
      "source": [
        "FOXNews = getFOXNewsDict(FOXNewsLinks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 214,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "100"
            ]
          },
          "execution_count": 214,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(FOXNews)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Get CNN news dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 248,
      "metadata": {},
      "outputs": [],
      "source": [
        "def getCNNNewsDict(url_list):\n",
        "\n",
        "  # Your key should be the news title and value should be the article text of the news.\n",
        "  newsDict = {}\n",
        "  # IMPLEMENT YOUR CODE HERE:# \n",
        "  for url in url_list:\n",
        "    article = Article(url)\n",
        "    article.download()\n",
        "    article.parse()\n",
        "    title = article.title\n",
        "    text = article.text\n",
        "    if title != '' and text != '':\n",
        "        newsDict[title] = text\n",
        "    \n",
        "    if len(newsDict) == 100:\n",
        "        break\n",
        "  \n",
        "  return newsDict\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 249,
      "metadata": {},
      "outputs": [],
      "source": [
        "CNNNews = getCNNNewsDict(CNNNewsLinks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 269,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "26"
            ]
          },
          "execution_count": 269,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(CNNNews)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Still not enough"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 268,
      "metadata": {},
      "outputs": [],
      "source": [
        "import newspaper\n",
        "cnn_paper = newspaper.build('http://cnn.com')\n",
        "for article in cnn_paper.articles:\n",
        "    article.download()\n",
        "    article.parse()\n",
        "    title = article.title\n",
        "    text = article.text\n",
        "    if title != '' and text != '':\n",
        "        CNNNews[title] = text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 275,
      "metadata": {},
      "outputs": [],
      "source": [
        "CNNNews2 = getCNNNewsDict(CNNNewsLinks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 278,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "22"
            ]
          },
          "execution_count": 278,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(CNNNews2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 279,
      "metadata": {},
      "outputs": [],
      "source": [
        "CNNNews.update(CNNNews2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 280,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "29"
            ]
          },
          "execution_count": 280,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(CNNNews)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 281,
      "metadata": {},
      "outputs": [],
      "source": [
        "ABCNews = getFOXNewsDict(ABCNewsLinks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 284,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "86"
            ]
          },
          "execution_count": 284,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ABCNews.update(CNNNews)\n",
        "len(ABCNews)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 296,
      "metadata": {},
      "outputs": [],
      "source": [
        "suplementLinks = FOXNewsLinks.reverse()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 297,
      "metadata": {},
      "outputs": [],
      "source": [
        "suplementDict = getFOXNewsDict(FOXNewsLinks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 300,
      "metadata": {},
      "outputs": [],
      "source": [
        "ABCNews.update(suplementDict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "add ABC FOX CNN but totally different from the pure Fox one"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 301,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "100"
            ]
          },
          "execution_count": 301,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(ABCNews)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "save file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 303,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Lastly, write them down in a .csv file for both the abc and fox news. \n",
        "\n",
        "import csv\n",
        "\n",
        "pathToSave = 'newsContents.csv'\n",
        "\n",
        "# size check\n",
        "assert len(ABCNews)>=100 and len(FOXNews)>=100, \"the size of both news dictionary should be no less than 100. got {} for abc news and {} for fox news instead.\".format(len(ABCNews),len(FOXNews))\n",
        "\n",
        "with open(pathToSave, 'w', newline='') as csvfile:\n",
        "  fieldnames = ['idx','newsSource','newsTitle','newsContents']\n",
        "  writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "  writer.writeheader()\n",
        "  for i,newsDictKey in enumerate(ABCNews.keys()):\n",
        "    writer.writerow({'idx': i,'newsSource':'ABCNews', 'newsTitle': newsDictKey,'newsContents': ABCNews[newsDictKey]})\n",
        "  for i,newsDictKey in enumerate(FOXNews.keys()):\n",
        "    writer.writerow({'idx': i,'newsSource':'FoxNews', 'newsTitle': newsDictKey,'newsContents': FOXNews[newsDictKey]})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 304,
      "metadata": {
        "id": "Ao9blGHEYVFb"
      },
      "outputs": [],
      "source": [
        "# Here is a function you could load the text data if your saved data follows\n",
        "# the format we provide the in first lab section code.\n",
        "\n",
        "def loadNewsTexts(csvPath):\n",
        "\n",
        "  # the function returns two dictionaries, one for ABC news text data and one for Fox news text data\n",
        "\n",
        "  abcNewsRawTextDict = {}\n",
        "  foxNewsRawTextDict = {}\n",
        "  with open(csvPath, newline='') as csvfile:\n",
        "    reader = csv.DictReader(csvfile)\n",
        "    for row in reader:\n",
        "      if (row['newsSource'] == \"ABCNews\"):\n",
        "        abcNewsRawTextDict[row['newsTitle']] = row['newsContents']\n",
        "      else:\n",
        "        foxNewsRawTextDict[row['newsTitle']] = row['newsContents']\n",
        "\n",
        "  return abcNewsRawTextDict,foxNewsRawTextDict\n",
        "\n",
        "# Load your news text data here\n",
        "# abcNewsDict,foxNewsDict = loadNewsTexts('./newsContents.csv')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 306,
      "metadata": {},
      "outputs": [],
      "source": [
        "abcNewsRawTextDict,foxNewsRawTextDict = loadNewsTexts('newsContents.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 308,
      "metadata": {},
      "outputs": [],
      "source": [
        "def preprocess(wikiTextDict):\n",
        "  # Input: a wiki text dictionary with keys are titles and values are the corresponding texts.\n",
        "  # Output: a wiki text dictionary with keys are the titles and the values are the preprocessed texts \n",
        "  # (sentences - tokens).\n",
        "\n",
        "  # sub-task 1: remove all the references texts \"[...]\"\n",
        "\n",
        "  nlp = spacy.load(\"en_core_web_sm\")\n",
        "  lemmatizer = nlp.get_pipe(\"lemmatizer\")\n",
        "  for key in wikiTextDict.keys():\n",
        "    wikiTextDict[key] = re.sub(r'[\\[].*?[\\]]', '', wikiTextDict[key])\n",
        "    \n",
        "  # sub-task 2: segment all the sentences in the wiki texts.\n",
        "    sentences = []\n",
        "    docParagraph = nlp(wikiTextDict[key])\n",
        "    assert docParagraph.has_annotation(\"SENT_START\")\n",
        "    for sent in docParagraph.sents:\n",
        "      # sub-task 3: tokenize the sentences from sub-task 2.\n",
        "      # sub-task 4: lemmatize the tokens from sub-task 3.\n",
        "      # sub-task 5: lower-case the tokens from sub-task 3/4.\n",
        "\n",
        "      tokens = []\n",
        "      docSent = nlp(sent.text)\n",
        "      for token in docSent:\n",
        "        tokens.append(token.lemma_.lower())\n",
        "      sentences.append(tokens)\n",
        "    wikiTextDict[key] = sentences\n",
        "\n",
        "  return wikiTextDict\n",
        "  # You don't need to follow the order of the sub-tasks.\n",
        "abcProcessed = preprocess(abcNewsRawTextDict)\n",
        "foxProcessed = preprocess(foxNewsRawTextDict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qcUECI1TYVWo"
      },
      "source": [
        "Task 12. Construct a histogram of word count from both sources. The X-axis should be unique words in decending order of word count and the Y-axis should be the counts for each word.\n",
        "\n",
        "(Please remember to preprocess the text data first.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 310,
      "metadata": {},
      "outputs": [],
      "source": [
        "def computeFreq(wikiTextDict):\n",
        "\n",
        "  # Input: a wiki text dictionary with keys are titles and values are the preprocessed corresponding texts.\n",
        "  # Output: a dictionary with keys are the word types, and the values are the appearance counts of the word types\n",
        "  tokenDict = {}\n",
        "  for key in wikiTextDict.keys():\n",
        "    for sentence in wikiTextDict[key]:\n",
        "      for token in sentence:\n",
        "        if token.isalpha() == True:\n",
        "          if token in tokenDict.keys():\n",
        "            tokenDict[token] += 1\n",
        "          else:\n",
        "            tokenDict[token] = 0\n",
        "  return tokenDict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 321,
      "metadata": {},
      "outputs": [],
      "source": [
        "def computeWords(freqDict,stop_words):\n",
        "  \n",
        "  # Input: a dictionary with keys are the word types, and the values are the appearance counts of the word types\n",
        "  # Output: a list of 20 words that appear most frequently in all the preprocessed scraped texts.\n",
        "\n",
        "  # If not preprocessed specifically, the punctuations still exist in the frequency dictionary from task 3.\n",
        "  # You need to remove them before outputing the top 20 words.\n",
        "  sortKey = []\n",
        "  sortValue = []\n",
        "  sortedList = sorted(freqDict.items(), key=lambda dict: dict[1], reverse=True)\n",
        "  for item in sortedList:\n",
        "    if item[0] not in stop_words:\n",
        "      sortKey.append(item[0])\n",
        "      sortValue.append(item[1])\n",
        "  return sortKey,sortValue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 322,
      "metadata": {
        "id": "V9gQ7NL-YllR"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# Preprocess the news data.\n",
        "# Compute word type list and the word token list.\n",
        "abcFreq = computeFreq(abcProcessed)\n",
        "abcWord,abcCount = computeWords(abcFreq,stop_words)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 324,
      "metadata": {},
      "outputs": [],
      "source": [
        "foxFreq = computeFreq(foxProcessed)\n",
        "foxWord,foxCount = computeWords(foxFreq,stop_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'abcWord' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn [1], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m     plt\u001b[38;5;241m.\u001b[39mhist(wordType,bins\u001b[38;5;241m=\u001b[39mwordTokens)\n\u001b[1;32m     10\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[0;32m---> 12\u001b[0m plotHistogram(\u001b[43mabcWord\u001b[49m,abcCount)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Plot the histogram here.\u001b[39;00m\n\u001b[1;32m     14\u001b[0m plotHistogram(foxWord,foxCount)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'abcWord' is not defined"
          ]
        }
      ],
      "source": [
        "\n",
        "def plotHistogram(wordType,wordTokens):\n",
        "  # Input: a list of word types, a list of word token counts to the corresponding word types\n",
        "  # Output: (display the histogram of word count from a news source)\n",
        "\n",
        "  # X-axis should be (indexes) of the word type, and Y-axis should be the word counts of the word type.\n",
        "  \n",
        "    plt.figure(figsize = (6, 4))\n",
        "    # for i in range(len(wordType)):\n",
        "    plt.hist(wordType,bins=wordTokens)\n",
        "    plt.show()\n",
        "\n",
        "plotHistogram(abcWord,abcCount)\n",
        "# Plot the histogram here.\n",
        "plotHistogram(foxWord,foxCount)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mx9B-pGyYmKH"
      },
      "source": [
        "Task 13. Construct the word clouds from the two texts. Include the word clouds and comment your interesting insights after that."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UlVltJhkYz_c"
      },
      "outputs": [],
      "source": [
        "# You may consider re-use the code from task 5 here.\n",
        "\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plotWordCloud(image):\n",
        "  # Input: word cloud image\n",
        "  # Output: (display the cloud image in the output)\n",
        "\n",
        "  plt.figure(figsize=(40, 30))\n",
        "  # Display image\n",
        "  plt.imshow(image) \n",
        "  # No axis details\n",
        "  plt.axis(\"off\")\n",
        "\n",
        "def generateWordCloud(text):\n",
        "  # Input: all texts in the scraped wiki data.\n",
        "  # Output: word cloud image.\n",
        "  wordcloud = WordCloud(width= 3000, height = 2000, random_state=1, background_color='salmon', colormap='Pastel1', collocations=False, stopwords = STOPWORDS).generate(text)\n",
        "\n",
        "  return wordcloud\n",
        "\n",
        "# Draw your word cloud here\n",
        "\n",
        "allWordList = []\n",
        "for paragraph in abcProcessed.values():\n",
        "    for sent in paragraph:\n",
        "        for word in sent:\n",
        "            if word not in stop_words and word.isalpha() == True:\n",
        "                allWordList.append(word)\n",
        "\n",
        "len(allWordList)\n",
        "\n",
        "wordcloud = generateWordCloud(' '.join(allWordList))\n",
        "plotWordCloud(wordcloud)\n",
        "\n",
        "\n",
        "allWordList = []\n",
        "for paragraph in foxProcessed.values():\n",
        "    for sent in paragraph:\n",
        "        for word in sent:\n",
        "            if word not in stop_words and word.isalpha() == True:\n",
        "                allWordList.append(word)\n",
        "\n",
        "len(allWordList)\n",
        "\n",
        "wordcloud = generateWordCloud(' '.join(allWordList))\n",
        "plotWordCloud(wordcloud)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "My m2 chip macbook cannot install the wordcloud because some fitting problem, I've checked on the colab, it's worked.  \n",
        "And the colab failed to install the newspaper lib, so sorry about no Pics in there. You can check the code in a feasible environment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "From the wordcloud, the news very like reference from someones comments because there are lots of \"say\". Means often use someone says that...."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.6 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
